{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus assignement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- Your submission should be the `.ipynb` file with your name,\n",
    "  like `FirstNameLastName.ipynb`. It should include the answers to the questions in\n",
    "  markdown cells.\n",
    "- You are expected to follow the best practices for code writing and model\n",
    "training. Poor coding style will be penalized.\n",
    "- You are allowed to discuss ideas with your peers, but no sharing of code.\n",
    "Plagiarism in the code will result in failing. If you use code from the\n",
    "internet, cite it. \n",
    "- Read each instruction carefully and provide complete answers to each question/task\n",
    "- You are allowed to use Keras or Pytorch \n",
    "\n",
    "> **_NOTE:_**  Write your email address in the cell below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m.rabotiaev@innopolis.university"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I- Open questions (3 points)\n",
    "\n",
    "Read [this article](https://link.springer.com/referenceworkentry/10.1007/978-0-387-73003-5_304) and answer the following questions:\n",
    "\n",
    "1. What is incremental learning?\n",
    "    - Is the model in which the input data is continiously used to train the model, and extend its' knowledge of the data. It is considered to be a dynamic technique of combining the supervised and unsupervised learning, and can be applied when the data becomes available gradually. \n",
    "2. Why is it important for us to create neural networks that would someday be able to learn incrementally?\n",
    "    - There are five main reasons why it is important. First of all, it might not be possible for us to store all the inforation, that we need in order to train our model. And even if we will be able to get all the necessary data, our model, may nont be able to deal with at once, and we will need to feed a data to it sequentially. Also, with the availability of new examples learning from scratch might waste time and computation resource. Another reason is, if the example generation is time-dependent that it suit the incremental learning style. And last, but not least the \"concept shift\", the learner should be able to self adapt to the learning environmnet. \n",
    "3. What is catastrophic forgetting?\n",
    "     - It is the tendency of the neural net to abruptly and drastically forget previously obtained information, when learning a new one. And hence the goal of incremental learning is to avoid such catastrophic forgetting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II- Train simple CNN model for digit classification (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "- Load MNIST dataset and split it in **Tr**ainning (`Tr`) and **Te**ting set (`Te`), 80% and 20% respectively.\n",
    "- Train a simple CNN for digit classification on the training set. \n",
    "- After fine tuning your CNN, evaluate the `overall` and the `class-wise` performances on `Te`. \n",
    ">**NOTE:** For the class-wise performance, you should plot (e.g., bar plots) the performance of your model on each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 18:01:44.020838: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 13s 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 18:02:30.357209: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 55s 143ms/step - loss: 0.2662 - accuracy: 0.9190 - val_loss: 0.0729 - val_accuracy: 0.9787\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 56s 149ms/step - loss: 0.0932 - accuracy: 0.9726 - val_loss: 0.0528 - val_accuracy: 0.9839\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 49s 130ms/step - loss: 0.0670 - accuracy: 0.9797 - val_loss: 0.0495 - val_accuracy: 0.9861\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 51s 135ms/step - loss: 0.0554 - accuracy: 0.9832 - val_loss: 0.0439 - val_accuracy: 0.9874\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 50s 134ms/step - loss: 0.0468 - accuracy: 0.9849 - val_loss: 0.0441 - val_accuracy: 0.9868\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 55s 148ms/step - loss: 0.0391 - accuracy: 0.9876 - val_loss: 0.0410 - val_accuracy: 0.9877\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 53s 142ms/step - loss: 0.0362 - accuracy: 0.9885 - val_loss: 0.0444 - val_accuracy: 0.9876\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 60s 160ms/step - loss: 0.0327 - accuracy: 0.9893 - val_loss: 0.0433 - val_accuracy: 0.9893\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 48s 128ms/step - loss: 0.0296 - accuracy: 0.9905 - val_loss: 0.0397 - val_accuracy: 0.9887\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 53s 141ms/step - loss: 0.0258 - accuracy: 0.9918 - val_loss: 0.0395 - val_accuracy: 0.9896\n",
      "Test loss: 0.030797217041254044\n",
      "Test accuracy: 0.9912999868392944\n",
      "313/313 [==============================] - 2s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_val = x_train[:12000]\n",
    "y_val = y_train[:12000]\n",
    "x_train = x_train[12000:]\n",
    "y_train = y_train[12000:]\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print the overall and class-wise performance\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Print the class-wise performance\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III- Create different tasks from the MNIST dataset (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split `Tr` into 3 datasets (tasks) according to the following distribution.\n",
    "\n",
    "- Task 1 contains digits of classes 0, 1, and 2. \n",
    "- Task 2 contains classes 3, 4, and 5. \n",
    "- Task 3 contains classes 6, 7, 8, and 9.\n",
    " \n",
    "*The following picture showcases the general scheme*\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?id=1vdDgdN9BGQ2Jl3Yg4YiPvfb5fcAeJZJ-' style=\"width:500px;\"> \n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# load the data from mnist \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create our three tasks1\n",
    "dataset1_x = []\n",
    "dataset1_y = []\n",
    "dataset2_x = []\n",
    "dataset2_y = []\n",
    "dataset3_x = []\n",
    "dataset3_y = []\n",
    "\n",
    "# append to different datasets\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] in [0, 1, 2]:\n",
    "        dataset1_x.append(x_train[i])\n",
    "        dataset1_y.append(y_train[i])\n",
    "    elif y_train[i] in [3, 4, 5]:\n",
    "        dataset2_x.append(x_train[i])\n",
    "        dataset2_y.append(y_train[i])\n",
    "    else:\n",
    "        dataset3_x.append(x_train[i])\n",
    "        dataset3_y.append(y_train[i])\n",
    "\n",
    "\n",
    "# Reshape and normalize the input data\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8ca30e2bccd57a437bb1de6eac09d75593a8bc5963996b389c9626968a1ab4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
